部分来源: 极客时间MySQL实战45讲 丁奇 和NaNrailgun 

#**三大范式**

第一范式 -----> 表的每一列都要保持它的原子性，也就是列不能再被分割。

1NF是对属性的**原子性**

第二范式 -----> 有主键 非主键列都需要依赖主键。

2NF是对记录的**惟一性**

第三范式 -----> 非主键列不能依赖于其他非主键列。

3NF是对字段的**冗余性**，要求任何字段不能由其他字段派生出来，它要求字段没有冗余，即不存在传递依赖；

#**架构**

1. 连接器与客户端建立TCP连接，验证账号密码。如果认证通过则查找这个用户的权限，正式建立这个数据库连接。（这个连接就依赖此时读到的权限。其中管理员修改了权限需要重新登录才能生效)     连接分为长连接短链接（连接为MySQL抽象出来的一个能让我们操作数据库服务器的接口，其本质还是TCP连接）。长连接：客户端的操作的复用这条连接。短链接：每次执行完很少的几次操作就断开连接，下次查询再建立。（MySQL会将一些数据绑定到连接对象中，所以一个连接复用太久的话会导致内存占用过大，解决方法是关掉重连或者reset命令让连接恢复到刚建立的样子）mysql_reset_connection
2. 缓存，8.0之后无查询缓存了
3. 分析器分析SQL语句，分析语法是否符合规范。
4. 优化器，有多种执行方案能实现目标，系统通过优化器选择最优方案。
5. 执行器，先判断下有没有权限，之后根据优化器提供的最优方案调用引擎接口。

#**日志**

MySQL 里经常说到的 WAL 技术—— Write-Ahead Logging，先写日志，再写磁盘

- **redolog**

innodb实现的物理日志，用来**保证数据库异常重启时数据不丢失**和**提升数据修改时的效率**

InnoDB 的 redo log 是固定空间大小的，循环写。写满了，就擦掉最久	的部分。

有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe

> 刷脏时机？
>
> 1. 内存不够（LRU）
> 2. redolog不够写
> 3. 数据库空闲
> 4. 正常关闭
>
> 刷脏影响性能：一个查询需要淘汰的脏页个数太多，会导致查询过慢 && redolog写满，更新全部堵住
>
> redolog刷脏变快的原因：
>
> （1）直接刷脏是随机IO，因为每次修改的数据位置随机，但写redo log是追加操作，属于顺序IO。
>
> （2）直接刷脏是以数据页（Page）为单位的，MySQL默认页大小是16KB，一个Page上一个小修改都要整页写入；而redo log中只包含真正需要写入的部分，无效IO大大减少。

- **binlog**

binlog，是Server层实现的，适用于所有引擎，用来记录所有**表数据的修改和表结构变更**的逻辑日志，主要用于主从复制和增量恢复

binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。

| 模式              | 内容                           | 优势                                                         | 劣势                                                         |
| ----------------- | ------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| statement（默认） | 记录引起数据变更的SQL语句      | 没有记录实际数据，日志量少，性能好                           | 类似uuid这样的函数每次执行的结果都是不确定的，使用statement的binlog进行回放必然出现数据不一致 |
| row               | 记录每次变更数据的前后两次数据 | 可以绝对精准的还原                                           | 会导致binlog的体积很大，对于修改记录的字段大的操作来说记录日志的性能损耗很大 |
| mixed             | statement和row两种模式混合     | 一般操作使用statement记录，涉及uuid等不确定的结果使用row模式 |                                                              |

- **两阶段提交**(一条更新语句是怎么执行的)

给ID=2 的某个同行的字段属性加1，比如c     update T set c=c+1 where ID=2;

1、执行器先找到引擎ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一行所在的数据页本来就在内存中，就是直接返回执行器；否则，需要先从磁盘读入内存，然后再返回。

2、执行器拿到了引擎给的行数据，把c值加1，得到新的一行数据，再调用引擎接口写入这行数据。

3、当引擎拿到这行数据，它会将这行数据更新到内存中，同时将这个更新操作记录到redo log里面，此时redo log处于prepare状态。然后告知执行器执行完成了，随时可以提交事务。

4、执行器会生成这个操作的binlog，并把binlog写入磁盘。

5、执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交状态，更新完成

> 1 prepare阶段 2 写binlog 3 commit
> 当在2之前崩溃时
> 重启恢复：后发现没有commit，回滚。备份恢复：没有binlog 。
> 一致
> 当在3之前崩溃
> 重启恢复：虽没有commit，但满足prepare和binlog完整，所以重启后会自动commit。备份：有binlog. 一致

* **undolog 回滚日志**

1. insert undo log ：插入新纪录的时候产生的undolog，只在事务回滚时需要，在事务提交之后就能丢弃。
2. update undo log ：更新或删除数据时产生的undolog，在事务回滚的时候需要，在快照读的时候也需要，只有在快照读和事务回滚不涉及该日志的时候才能被删除。

* **乐观锁和悲观锁**

>悲观锁（数据库）
>
>--共享锁：事务A对某数据加了共享锁之后，其它事务只能对该数据加共享锁，但不能加排他锁
>
>--排他锁：事务A对某数据加了排他锁之后，其它事务对该数据不能加共享锁也不能加排他锁。
>
>乐观锁
>
>--版本号、时间戳

#**ACID**

**a：原子性** 【要么全部成功要么全部失败】

原理是使用commit和rollback实现，Innodb使用undoLog实现回滚，如果正在进行一个事务，Innodb会把这个事务的修改记录到undoLog中，如果需要回滚则使用这个事务的undoLog进行事务回滚。

**d：持久性** 【一旦提交永久保存】

Innodb使用redolog实现持久性。MySQL对于数据的修改，先写内存和redolog再用binlog写进磁盘。当数据库宕机的时候能通过redolog恢复，保证了持久性。

**I：隔离性**（脏写在所有隔离级别下都得避免）【并发执行的各个事务之间不能相互干扰】

读未提交（允许事务读取未被其他事务提交的变更数据）【**读不加锁**。写加锁，事务结束释放锁】

读提交（只允许事务读取已经被其他事务提交的变更数据）【每次读sql时新建一个快照，已提交的事务可见，其他不可见。写加锁，事务结束释放锁】

可重复读（一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的）【读，事务开启时快照。写加锁，使用next-key锁，之前两种都是行锁，事务结束解锁）（MySQL快照读使用mvcc和当前读next-key lock解决了幻读】

串行化 【“写”会加“写锁”，“读”会加“读锁”】

![image-20210206204753704](C:\Users\11468\AppData\Roaming\Typora\typora-user-images\image-20210206204753704.png)

Y为发生N为不发生

**c：一致性**

通过以上三点保证一致性和业务层面保证一致性。

#**事务**

##脏读、不可重复读、幻读

第一类丢失更新：两个事务更新同一条数据资源，某一事务完成，另一事务异常终止，回滚造成前面一个事务完成的更新也同时丢失 。

脏读:可以读到其他事务未提交的数据； 

不可重复读：在事务中前后读取的记录内容不一致； 

幻读：刚开始读发现不存在然后插入，但是发现插入失败导致在事务中前后读取的记录数量（集合）不一致——“幻读”（rr模式下引入了间隙锁解决当前读下的幻读问题，快照读是mvcc解决的）。幻读是因为新插入/新删除导致新增了满足/不满足当前查询条件的行，导致前后查询结果不一致，数据库发生幻觉。

第二类丢失更新：在不可重复读有一种特殊情况，两个事务更新同一条数据资源，后完成的事务会造成先完成的事务更新丢失

##事务隔离级别：

1. 读未提交是指，允许事务读取未被其他事务提交的变更数据。不能防止第一类更新丢失问题，不能解决脏读，不可重复读及幻读问题
2. 读提交是指，只允许事务读取已经被其他事务提交的变更数据。可以防止脏读问题，但会出现不可重复读及幻读问题
3. 可重复读（默认）是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。（当然，后来引入了间隙锁解决当前读的幻读问题）
4. 串行化。当出现读写锁冲突的时候，串行执行。可以解决上面提到的所有并发问题，但可能导致大量的超时现象和锁竞争

##**当前读和快照读**

**当前读**

　　select...lock in share mode (共享读锁)
　　select...for update
　　update , delete , insert

　　当前读, 读取的是最新版本, 并且**对读取的记录加锁, 阻塞其他事务同时改动相同记录**，避免出现安全问题。

　　例如，假设要update一条记录，但是另一个事务已经delete这条数据并且commit了，如果不加锁就会产生冲突。所以update的时候肯定要是当前读，得到最新的信息并且锁定相应的记录。

【当前读的实现方式：next-key锁/临键锁 (**行记录锁**+**Gap间隙锁**】

**快照读**

　　单纯的select操作，**不包括**上述 select ... lock in share mode, select ... for update。　　　　

　　Read Committed隔离级别：每次select都生成一个快照读。

　　Read Repeatable隔离级别：**开启事务后第一个select语句才是快照读的地方，而不是一开启事务就快照读。**

【快照读的实现方式：**undolog**和多版本并发控制**MVCC**】

InnoDB 给每一个事务生成一个唯一事务 ID 的方法称为生成快照，因此这种场景称为**快照读**。

##MVCC

多版本并发控制（MVCC）是一种用来解决读-写冲突的无锁并发控制，它给事务分配单向增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照。 所以MVCC可以为数据库解决以下问题

- 在并发读写数据库时，可以做到在读操作时不用阻塞写操作，写操作也不用阻塞读操作，提高了数据库并发读写的性能
- 同时还可以解决脏读，幻读（读时幻读，写的幻读没有解决），不可重复读等事务隔离问题，但不能解决更新丢失问题

mvcc是靠3个东西实现的，隐式字段（事务ID、回滚指针、隐藏自增字段），undo日志 ，Read View（系统生成的快照）实现的

注意：可重复读 -- 事务开始时创建ReadView. 读提交 -- 事务每个语句都创建新的ReadView

#**索引**

**B+树** 是一个多叉树，也是一个为磁盘而设计的一种平衡查找树，

一个索引一个B+树，非叶子节点存储的是key，叶子节点存储的key和value

- 当一个结点满时，分配一个新的结点，并将原结点中1/2的数据复制到新结点，最后在父结点中增加新结点的指针(页分裂）；小于50%就删除这个节点（页合并）
- 根节点的子节点个数可以不超过 m/2，这是一个例外

**主键索引**叶子节点： key:主键的值，value:整行数据。 

**普通列索引**叶子节点： key：索引列的值， value:主键的值。

在 InnoDB 中，我们通过**数据页之间通过双向链表连接**以及**叶子节点中数据之间通过单向链表连接的方式**可以找到表中所有的数据。

基于主键索引和普通索引的查询有什么区别？

如果语句是 select * from T where ID=500，即主键查询方式，则只需要搜索 ID 这棵 B+ 树；

如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表。也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。

1、**覆盖索引**：如果查询条件使用的是普通索引（或是联合索引的最左原则字段），**查询结果是联合索引的字段或是主键**，不用回表操作，直接返回结果，减少IO磁盘读写读取正行数据

2、最左前缀：联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。例如索引是key index (a,b,c). 可以支持【a】 【a,b】 【a,b,c】 3种组合进行查找，但不支持 【b,c】进行查找。

3、联合索引：根据创建联合索引的顺序，以最左原则进行where检索，比如（age，name）以age=1 或 age= 1 and name=‘张三’可以使用索引，单以name=‘张三’ 不会使用索引，考虑到存储空间的问题，还请根据业务需求，将查找频繁的数据进行靠左创建索引。

4、索引下推：like 'hello%’and age >10 检索，MySQL5.6版本之前，会对匹配的数据进行回表查询。5.6版本后，会先过滤掉age<10的数据，**把不满足的字段过滤掉，再进行回表，减少回表率，提升检索速度**。【索引下推】Index Condition Pushdown，简称 ICP。 是Mysql 5.6版本引入的技术优化。旨在 在“仅能利用最左前缀索的场景”下（而不是能利用全部联合索引），对不在最左前缀索引中的其他联合索引字段加以利用——在遍历索引时，就用这些其他字段进行过滤(where条件里的匹配)。**过滤会减少遍历索引查出的主键条数，从而减少回表次数，提示整体性能**。

5、**全表扫描**：通常在数据库中，对无索引的表进行查询一般称为全表扫描；然而有时候我们即便添加了索引，但当我们的SQL语句写的不合理的时候也会造成全表扫描。 使用null做为判断条件、使用or做为连接条件 、！= 或<>、只有模糊匹配时、使用in时(或not in) 使用count(*)时 

6、int做主键和uuid做主键，从插入、查询、空间大小来看int比uuid性能好，但是int做不到uuid能完成的表合并和分布式下不重复id的问题。

7、Mysql Innodb中的索引数据结构是 B+ 树，普通索引，也叫做辅助索引，叶子节点存放的是主键值。主键上的索引叫做聚集索引，表里的每一条记录都存放在主键的叶子节点上。

唯一索引本质上是辅助索引，然后加了唯一约束。

# 索引失效

1.不要使用**in,not in <>,is null ,is not null ，like,msyql函数操作符**，这样数据库会进行全表扫描，或者优化器觉得全表比走索引快
 1.1：要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引
 1.2：like:以%结尾可以使用，以%开头，索引失效
 1.3：如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引
 2.WHERE后面的条件顺序影响

#**聚簇索引和非聚簇索引**

innodb的主键就是聚簇索引，数据存储与索引放到了一块，找到索引也就找到了数据 数据的物理存放顺序与索引顺序是一致的

这里我们着重介绍 InnoDB 中的聚集索引和非聚集索引：

![image-20201220180829743](file://D:\Acan\canwork\Note-master\MySQL.assets\image-20201220180829743.png?lastModify=1615291090)

**①聚集索引（聚簇索引）**：以 InnoDB 作为存储引擎的表，表中的数据都会有一个主键，即使你不创建主键，系统也会帮你创建一个隐式的主键。

这是因为 InnoDB 是把数据存放在 B+ 树中的，而 B+ 树的键值就是主键，在 B+ 树的叶子节点中，存储了表中所有的数据。

这种以主键作为 B+ 树索引的键值而构建的 B+ 树索引，我们称之为聚集索引。

**②非聚集索引（非聚簇索引）：**以主键以外的列值作为键值构建的 B+ 树索引，我们称之为非聚集索引。

非聚集索引与聚集索引的区别在于非聚集索引的叶子节点不存储表中的数据，而是存储该列对应的主键，想要查找数据我们还需要根据主键再去聚集索引中进行查找，这个再根据聚集索引查找数据的过程，我们称为**回表**。

myISAM 主键和二级索引都是非聚集索引。只有节点中的索引字段不同，它们的叶子节点都是存储的指向数据行存储位置的指针。从这里就可以看出非聚集索引中的数据行数据在物理存储器中的真实位置和索引的逻辑顺序是不同的。且地址在磁盘上存储的位置不连续。

* 注意：MyISAM存储引擎只支持非聚集索引，它的非聚集索引的实现方式是是**叶子节点中存储指向数据行的指针**，可以直接取得全部的数据行数据，所以MyISAM表中的主键索引和二级索引都没有二次查询问题。

**用B+树不用B树的原因/区别**

（1）B树必须用中序遍历的方法按序扫库 而B+ 树支持范围查找，排序查找，分组查找以及去重查找变得异常简单。而 B 树因为数据分散在各个节点，要实现这一点很困难。

（2）减少磁盘IO次数

因为 B 树不管叶子节点还是非叶子节点，都会保存数据，这样导致在非叶子节点中能保存的指针数量变少（有些资料也称为扇出），指针少的情况下要保存大量数据，只能增加树的高度，导致 IO 操作变多，查询性能变低。而B+树具体的数据都存在叶子节点上，可以让树更矮，能进一步压缩树的高度，减少磁盘IO次数。

当我们要在磁盘上索引一个记录时，将磁盘中的数据传输到内存中才是花费时间的地方，在内存中的索引过程所花的时间基本是可以忽略不计的。在磁盘中以B+树的形式组织数据就有着天然的优势。减少磁盘io次数。

* 注意（根节点是常驻内存的）（树高其实取决于叶子树（数据行数）和“N叉树”的N。 而N是由页大小和索引大小决定的。）

#**MySQL的锁**

![image-20210312222355425](C:\Users\11468\AppData\Roaming\Typora\typora-user-images\image-20210312222355425.png)



###**MDL**锁

MySQL5.5版本开始引入了MDL锁，来保护表的元数据信息，用于解决或者保证DDL操作（创建和管理表）与DML操作（crud）之间的一致性。

当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。

问题：因为MDL锁，会导致表级别的锁，无论是读或者写操作，都无法进行，导致SQL的阻塞。

办法：在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。然后再人为重试。或者在业务低峰期执行

### 记录锁（Record Locks）

顾名思义，记录锁就是为**某行**记录加锁，它`封锁该行的索引记录`：

```
-- id 列为主键列或唯一索引列
SELECT * FROM table WHERE id = 1 FOR UPDATE;
```

id 为 1 的记录行会被锁住。

需要注意的是：`id` 列必须为`唯一索引列`或`主键列`，否则上述语句加的锁就会变成`临键锁`。

同时查询语句必须为`精准匹配`（`=`），不能为 `>`、`<`、`like`等，否则也会退化成`临键锁`

在通过 `主键索引` 与 `唯一索引` 对数据行进行 UPDATE 操作时，也会对该行数据加`记录锁`：

```
-- id 列为主键列或唯一索引列
UPDATE SET age = 50 WHERE id = 1;
```

### 间隙锁（Gap Locks）

**间隙锁**基于`非唯一索引`，它`锁定一段范围内的索引记录`。**间隙锁**基于下面将会提到的`Next-Key Locking` 算法，请务必牢记：**使用间隙锁锁住的是一个区间，而不仅仅是这个区间中的每一条数据**。

```
SELECT * FROM table WHERE id BETWEN 1 AND 10 FOR UPDATE;
复制代码
```

即所有在`（1，10）`区间（左开右开区间）内的记录行都会被锁住，所有id 为 2、3、4、5、6、7、8、9 的数据行的插入会被阻塞，但是 1 和 10 两条记录行并不会被锁住。

除了手动加锁外，在执行完某些 SQL 后，InnoDB 也会自动加**间隙锁**，这个我们在下面会提到。

### 临键锁（Next-Key Locks）

Next-Key 可以理解为一种特殊的**间隙锁**，也可以理解为一种特殊的**算法**。通过**临建锁**可以解决`幻读`的问题。 每个数据行上的`非唯一索引列`上都会存在一把**临键锁**，当某个事务持有该数据行的**临键锁**时，会锁住一段**左开右闭区间**的数据。需要强调的一点是，`InnoDB` 中`行级锁`是基于索引实现的，**临键锁**只与`非唯一索引列`有关，在`唯一索引列`（包括`主键列`）上不存在**临键锁**。

**总结：**

**InnoDB** 中的`行锁`的实现依赖于`索引`，一旦某个加锁操作没有使用到索引，那么该锁就会退化为`表锁`。

**记录锁**存在于包括`主键索引`在内的`唯一索引`中，锁定单条索引记录。

**间隙锁**存在于`非唯一索引`中，锁定`开区间`范围内的一段间隔，它是基于**临键锁**实现的。

**临键锁**存在于`非唯一索引`中，该类型的每条记录的索引上都存在这种锁，它是一种特殊的**间隙锁**，锁定一段`左开右闭`的索引区间。

# [你了解MySQL的加锁规则吗？](https://www.cnblogs.com/nedulee/p/11838682.html)

注：加锁规则指的是next-key lock

> 加锁规则可以概括为：两个原则、两个优化和一个bug:
>
> 原则1:加锁的基本单位是next-key lock，前开后闭
>
> 原则2:查找过程中访问到的对象才会加锁
>
> 优化1:索引上的等值查询，给唯一索引加锁的时候，next-key lock退化成行锁
>
> 优化2:索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁
>
> 1个bug:唯一索引上的范围查询会访问到不满足条件的第一个值为止。
>
> ——丁奇

#[explain](https://www.jianshu.com/p/593e115ffadd)

| id   | select_type | table     | partitions | type  | possible_keys | key     | key_len | ref   | rows | filtered | Extra |
| ---- | ----------- | --------- | ---------- | ----- | ------------- | ------- | ------- | ----- | ---- | -------- | ----- |
| 1    | SIMPLE      | user_info | NULL       | const | PRIMARY       | PRIMARY | 8       | const | 1    | 100.00   | NULL  |

#**最左匹配原则**

最左匹配原则都是针对联合索引来说的，注意此时数据库依据联合索引**最左的字段来构建 B+ 树**。比如查(a,b,c)联合索引，那么**a 是有序的，而 b，c 都是无序的**。但是**当在 a 相同的时候，b 是有序的，b 相同的时候，c 又是有序的**。

#**in和exits的区别**

in在查询的时候，首先查询子查询的表 然后按照条件进行筛选。所以相对内表比较小或者子查询得出的结果集记录较少的时候，in的速度较快。

exits先查我们的主查询的表 然后按照条件进行筛选。所以相对外表比较小的时候，exits的速度较快。

#**union /union all**

union all为（直接将两个查询结果集合并）

union 运算结果为（两个结果集合并后进行去重/distinct）

union 跟 union all的一个性能区别是：union涉及到去重，所以，用到了临时表。

#**索引设计的原则**

1. 服从最左原则
2. 避免索引冗余
3. 选用区分度高的字段建索引

#MyISAM 和 InnoDB 的区别

1. InnoDB 支持**事务**，MyISAM 不支持事务。这是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一；
2. InnoDB 支持**外键**，而 MyISAM 不支持。对一个包含外键的 InnoDB 表转为 MYISAM 会失败；
3. InnoDB 是**聚集索引**，MyISAM 是非聚集索引。聚簇索引的文件存放在主键索引的叶子节点上，因此 InnoDB 必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。而 MyISAM 是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。
4. InnoDB 不保存表的具体**行数**，执行 select count(*) from table 时需要全表扫描。而MyISAM 用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快；
5. InnoDB **最小的锁粒度是行锁**，MyISAM 最小的锁粒度是表锁。一个更新语句会锁住整张表，导致其他查询和更新都会被阻塞，因此并发访问受限。这也是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一；

#**数据库中怎么避免死锁？**

1）以固定的顺序访问表和行。即按顺序申请锁，这样就不会造成互相等待的场面。

2）大事务拆小。大事务更倾向于死锁，如果业务允许，将大事务拆小。

3）在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁概率。

4）降低隔离级别。如果业务允许，将隔离级别调低也是较好的选择，比如将隔离级别从RR调整为RC，可以避免掉很多因为gap锁造成的死锁。

5）为表添加合理的索引。如果不走索引将会为表的每一行记录添加上锁，死锁的概率大大增大。

#join

（不一定驱动表必走全表扫描，也可走索引，看实际情况，注意这个误区）

显然，在sql优化中，小表驱动大表优于大表驱动小表

1.当使用left join时，左表是驱动表，右表是被驱动表
2.当使用right join时，右表时驱动表，左表是被驱动表
3.当使用join时，mysql会选择数据量比较小的表作为驱动表，大表作为被驱动表

结果集是以驱动表的结果作为基准，去匹配被驱动表，有就显示，没有就显示为null。

#慢查询

记录sql查询执行时间超过规定时间的sql

long_query_time 默认值是10s 默认情况下不启动

## 开启慢查询

方式一：my.ini/my.cnf

```xml
[mysqld]
slow_query_log=1
slow_query_log_file=/var/log/mysql/log-slow-queries.log
long_query_time=2
```

方式二：mysql数据库开启

```sql
show variables like 'long_query_time';
show variables like 'slow_query%'
```

##分析慢查询日志         

​    直接分析mysql慢查询日志 ,利用explain关键字可以模拟优化器执行SQL查询语句，来分析sql慢查询语句

​    例如：执行EXPLAIN SELECT * FROM res_user ORDER BYmodifiedtime LIMIT 0,1000

​    得到如下结果： 显示结果分析： 

select_type|table | type | possible_keys | key |key_len | ref | rows | Extra EXPLAIN列的解释：      

​    select_type： 表示对应行是简单还是复杂的查询

​    table  显示这一行的数据是关于哪张表的      

​    type  这是重要的列，显示连接使用了何种类型。从最好到最差的连接类型为const、eq_reg、ref、range、indexhe和ALL 

​    rows  显示需要扫描行数

​    key   使用的索引

​    Extra列 额外信息。(常见)

> **Using index**查询的列被索引覆盖，并且where筛选条件是索引的前导列，是性能高的表现。一般是使用了覆盖索引(索引包含了所有查询的字段)。对于innodb来说，如果是辅助索引性能会有不少提高

```csharp
mysql> explain select film_id from film_actor where film_id = 1;
```

> **Using where**查询的列未被索引覆盖，where筛选条件非索引的前导列

```csharp
mysql> explain select * from actor where name = 'a';
```

> **Using where Using index**

查询的列被索引覆盖，并且where筛选条件是索引列之一但是不是索引的前导列，意味着无法直接通过索引查找来查询到符合条件的数据

```csharp
mysql> explain select film_id from film_actor where actor_id = 1;
```

> **NULL**

查询的列未被索引覆盖，并且where筛选条件是索引的前导列，意味着用到了索引，但是部分字段未被索引覆盖，必须通过“回表”来实现，不是纯粹地用到了索引，也不是完全没用到索引

```csharp
mysql>explain select * from film_actor where film_id = 1;
```

> **Using index condition**

与Using where类似，查询的列不完全被索引覆盖，where条件中是一个前导列的范围；

```csharp
mysql> explain select * from film_actor where film_id > 1;
```

> **Using temporary**

mysql需要创建一张临时表来处理查询。出现这种情况一般是要进行优化的，首先是想到用索引来优化。

1. actor.name没有索引，此时创建了张临时表来distinct

```csharp
mysql> explain select distinct name from actor;
```

1. film.name建立了idx_name索引，此时查询时extra是using index,没有用临时表

```csharp
mysql> explain select distinct name from film;
```

##慢查询优化

（1）**索引**没起作用的情况

* 1、使用LIKE关键字的查询语句

​        在使用LIKE关键字进行查询的查询语句中，如果匹配字符串的第一个字符为“%”，索引不会起作用。只有“%”不在第一个位置索引才会起作用。

* 2、使用多列索引的查询语句

​        MySQL可以为多个字段创建索引。一个索引最多可以包括16个字段。对于多列索引，只有查询条件使用了这些字段中的第一个字段时，索引才会被使用。

（2）优化数据库结构

* 1、将字段很多的表分解成多个表 

​        对于字段比较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。

* 2、增加中间表

​        对于需要经常联合查询的表，可以建立中间表以提高查询效率。通过建立中间表，把需要经常联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询，以此来提高查询效率。

（3）分解关联查询

将一个大的查询分解为多个小查询是很有必要的。

​     很多高性能的应用都会对关联查询进行分解，就是可以对每一个表进行一次单表查询，然后将查询结果在应用程序中进行关联，很多场景下这样会更高效

（4）**优化LIMIT分页**

例如：select id,title from collect limit 90000,10;//查询10020条然后只返回最后20条

方法一：虑筛选字段（title）上加索引
            title字段加索引 （此效率如何未加验证）

方法二：先查询出主键id值

​           select id,title from collect where id>=(select id from collect order by id limit 90000,1) limit 10;

​           原理：先查询出90000条数据对应的**主键id的值**，然后直接通过该id的值直接查询该id后面的数据。（说白了利用两次覆盖索引）

方法三：“关延迟联”
            如果这个表非常大，那么这个查询可以改写成如下的方式：

​            Select news.id, news.description from news inner join (select id from news order by title limit 50000,5) as myNew using(id);

​            这里的“关延迟联”将大大提升查询的效率，它让MySQL扫描尽可能少的页面，获取需要的记录后再根据关联列回原表查询需要的所有列。这个技术也可以用在优化关联查询中的limit。

方法四：建立复合索引 acct_id和create_time

​            select * from acct_trans_log WHERE  acct_id = 3095  order by create_time desc limit 0,10

​           注意sql查询慢的原因都是:引起filesort

（5）**优化group by**：如果对 group by 语句的结果没有排序要求，要在语句后面加 order by null；

#分库分表、垂直拆分、水平拆分

## 分库

**数据库垂直拆分**：按**业务分库**，同组的放在一个相同的数据库

（优点：业务清晰，系统整合扩展容易，数据维护简单）

（缺点：部分业务无法join，只能通过接口方式解决；单库会限制，数据的扩展性和性能；分布式事务处理复杂）

**数据库水平拆分**：垂直拆分后**遇到单机瓶颈**之后，就可以考虑水平拆分了。之所以先垂直拆分才水平拆分，是因为垂直拆分后数据业务清晰而且单一，更加方便指定水平的标准。

可以理解为对数据行的切分，按照**某种规则分散**到分到多个库中

（优点：不存在单库大数据高并发的性能瓶颈）

（缺点：跨库join性能低，分片事务一致性难于解决，数据多次扩展难度大;）

两种方式共同的缺点：（1）当引入分布式事务的问题。（2）跨节点join的问题（3）跨节点合并排序分页问题

## 分表

**数据表垂直拆分**：数据表垂直拆分就是纵向地把表中的列分成多个表，把表从“宽”变“窄”。一般遵循以下几个点进行拆分：

- **冷热分离**，把常用的列放在一个表，不常用的放在一个表。
- 大字段列独立存放
- 关联关系的列紧密的放在一起

**数据表水平拆分**：表结构维持不变。对数据行的切分，按照某种规则分散到多个表中

##拆分原则

尽量不拆分；业务读取尽量少用多表join，尽量通过保证数据冗余性，分组避免跨库多表join；尽量避免分布式事务，单表拆分数据1000万以内

##切分方法

范围、枚举、时间、哈希取模（前期切分容易，后期扩容迁移难）、一致性哈希、指定。

* 普通哈希算法，使用特定的数据，如Redis的键或用户ID，再根据节点数量N使用公式：hash（key）% N 计算出哈希值，用来决定数据映射到哪一个节点上。

优点

这种方式的突出优点是简单性，常用于数据库的分库分表规则。一般采用预分区的方式，提前根据数据量规划好分区数

缺点 

当节点数量变化时，如扩容或收缩节点，数据节点映射关系需要重新计算，会导致数据的重新迁移。所以扩容时通常采用翻倍扩容，避免 数据映射全部被打乱，导致全量迁移的情况，这样只会发生50%的数据迁移。

* [一致性哈希算法](https://segmentfault.com/a/1190000017568190)：一致性哈希是将整个哈希值空间组织成一个==虚拟的圆环==

一致性哈希分区

一致性哈希的目的就是为了在节点数目发生改变时尽可能少的迁移数据，将所有的存储节点排列在收尾相接的Hash环上，每个key在计算Hash 后会==顺时针==找到临接的存储节点存放。==而当有节点加入或退 时，仅影响该节点在Hash环上顺时针相邻的后续节点==。

![clipboard.png](https://segmentfault.com/img/bVblSr3?w=1160&h=781)

优点

加入和删除节点只影响哈希环中顺时针方向的相邻的节点，对其他节点无影响。

缺点 

数据的分布和节点的位置有关，因为这些节点不是均匀的分布在哈希环上的，所以数据在进行存储时达不到均匀分布的效果。

* 虚拟槽分区

本质上还是第一种的普通哈希算法，把全部数据==**离散到指定数量的哈希槽**==中，把这些哈希槽按照节点数量进行了分区。这样因为==哈希槽的数量的固定的==，添加节点也不用把数据迁移到新的哈希槽，只要在节点之间互相迁移就可以了，即保证了数据分布的均匀性，又保证了在添加节点的时候不必迁移过多的数据。

Redis的集群模式使用的就是虚拟槽分区，一共有16383个槽位平均分布到节点上

![clipboard.png](https://segmentfault.com/img/bVblSr2?w=507&h=567)

